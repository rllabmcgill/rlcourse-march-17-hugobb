{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Universal Value Function Approximators](http://jmlr.org/proceedings/papers/v37/schaul15.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen in class how value function approximation can help generalization across states. But can we push this idea even further ?\n",
    "\n",
    "Let's imagine we've trained an agent on a gridworld task, and found the optimal value function and thus the optimal policy. Now what if we change the reward position ? If the new goal is not too far from the original one we can probably start from the previous value function and learn quickly the optimal policy. However we if we changed back to the orginal goal, we have to re-learn again. What if we could remember what we've learn from the previous goal, and generalize our value function to a new goal ? That's the idea of Universal Value Function Approximators.\n",
    "\n",
    "Instead of defining $V_\\pi(s)$, we define $V_\\pi(s,g)$ (the value function also depends on the goal). This matrix can grow very quickly that's why we need value function approximation.\n",
    "\n",
    "We first defined a \"two-stream\" architecture based on matrix factorization, with two-stages:\n",
    "1. We lay out all the value $V_\\pi(s,g)$ in a matrix, we then find a low rank approximation of this matrix $V_\\pi(s,g)=\\phi_s^T\\psi_g$, where $\\phi_s$ is $r\\times N_s$ and $\\psi_g$ is $r\\times N_g$ with $N_s$ the state space dimension, $N_g$ the goal space dimension and $r$ a small rank. This decomposition can be obtained by SVD (Singular Value Decomposition).\n",
    "2. We learn two functions $\\phi$ and $\\psi$ via separate multivariate regression toward the targets $\\phi_s$ and $\\psi_g$ from phase 1.\n",
    "\n",
    "We tried this approach on a gridworld task with 4 rooms. The states and goals were represented by a 1-hot encoding corresponding to their positions in the grid. $\\phi$ and $\\psi$ are a simple dot product of the state and goal representation with matrix $\\theta_\\phi$ and $\\theta_\\psi$. The approach is trained on a random selection of half the goals and tested on the other half for generalization in a supervised learning fashion. The policy we evaluate is a uniform policy over actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import linalg\n",
    "from lasagne.layers import InputLayer, DenseLayer, get_output, get_all_params\n",
    "from lasagne import nonlinearities\n",
    "from lasagne.updates import adam\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import time\n",
    "\n",
    "filename = 'data.npy'\n",
    "rank = 12\n",
    "lr = 0.005\n",
    "n_epochs = 10000\n",
    "batch_size = 20\n",
    "seed = 1234\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset of precomputed value function for every state and goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "V = np.load(filename).astype(theano.config.floatX)\n",
    "idx = np.random.permutation(V.shape[1])\n",
    "train_idx, test_idx = list(idx[:int(V.shape[1]/2)]), list(idx[int(V.shape[1]/2):])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low rank approximation with SVD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M_train = np.zeros(V.shape, dtype=theano.config.floatX)\n",
    "M_train[:,train_idx] = V[:,train_idx]\n",
    "\n",
    "u, s, v = linalg.svds(M_train,k=rank)\n",
    "phi_s = u\n",
    "phi_g = np.dot(np.diag(s), v).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling...\n",
      "0.54 seconds for compiling.\n"
     ]
    }
   ],
   "source": [
    "true_state_emb = T.matrix()\n",
    "state_batch = T.matrix()\n",
    "state_network = InputLayer(shape=(None, V.shape[0]))\n",
    "state_network = DenseLayer(state_network, rank, nonlinearity=nonlinearities.linear)\n",
    "state_emb = get_output(state_network, inputs=state_batch)\n",
    "loss_state = T.sum((true_state_emb - state_emb)**2, axis=1).mean()\n",
    "params_state = get_all_params(state_network)\n",
    "updates_state = adam(loss_state, params_state, learning_rate=lr)\n",
    "\n",
    "true_goal_emb = T.matrix()\n",
    "goal_batch = T.matrix()\n",
    "goal_network = InputLayer(shape=(None, V.shape[1]))\n",
    "goal_network = DenseLayer(goal_network, rank, nonlinearity=nonlinearities.linear)\n",
    "goal_emb = get_output(goal_network, inputs=goal_batch)\n",
    "loss_goal = T.sum((true_goal_emb - goal_emb)**2, axis=1).mean()\n",
    "params_goal = get_all_params(goal_network)\n",
    "updates_goal = adam(loss_goal, params_goal, learning_rate=lr)\n",
    "\n",
    "print \"Compiling...\"\n",
    "t = time.time()\n",
    "train_state_emb = theano.function([true_state_emb, state_batch], [loss_state, state_emb], updates=updates_state)\n",
    "train_goal_emb = theano.function([true_goal_emb, goal_batch], [loss_goal, goal_emb], updates=updates_goal)\n",
    "test_state_emb = theano.function([state_batch], state_emb)\n",
    "test_goal_emb = theano.function([goal_batch], goal_emb)\n",
    "print '%.2f seconds for compiling.'%(time.time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training value function approximator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_hist_train = []\n",
    "mean_hist_test = []\n",
    "for i in range(n_epochs):\n",
    "    s = np.zeros((batch_size,V.shape[0]), dtype=theano.config.floatX)\n",
    "    s_idx = np.random.permutation(V.shape[0])[:batch_size]\n",
    "    s[np.arange(batch_size),s_idx] = 1\n",
    "    loss_s, state_emb = train_state_emb(phi_s[s_idx], s)\n",
    "\n",
    "    g = np.zeros((batch_size,V.shape[1]), dtype=theano.config.floatX)\n",
    "    g_idx = np.random.permutation(train_idx)[:batch_size]\n",
    "    g[np.arange(batch_size),g_idx] = 1\n",
    "    loss_g, goal_emb = train_goal_emb(phi_g[g_idx], g)\n",
    "\n",
    "    s = np.identity(V.shape[0], dtype=theano.config.floatX)\n",
    "    state_emb = test_state_emb(s)\n",
    "\n",
    "    g = np.identity(V.shape[1], dtype=theano.config.floatX)\n",
    "    goal_emb = test_goal_emb(g)\n",
    "    V_test = state_emb.dot(goal_emb.T)\n",
    "    mse = (V - V_test)**2\n",
    "    mean_hist_train.append(mse[:,train_idx].mean())\n",
    "    mean_hist_test.append(mse[:,test_idx].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664c022d61434474b6d5fe9f6d7d66e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b77099f626487dad7dddc4af5b688e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c85977e78a894ece816148f4fe42ceab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe101fc4490>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "plt.figure(1, figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "plt.title('MSE on training set')\n",
    "plt.loglog(np.arange(len(mean_hist_train)), mean_hist_train)\n",
    "plt.subplot(122)\n",
    "plt.title('MSE on validation set')\n",
    "plt.loglog(np.arange(len(mean_hist_train)), mean_hist_test)\n",
    "\n",
    "plt.figure(2, figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "plt.title('Ground Truth for Value Function on Training Set')\n",
    "plt.imshow(V[:,train_idx], cmap='coolwarm')\n",
    "plt.subplot(122)\n",
    "plt.title('Reconstruction of Value Function on Training Set')\n",
    "plt.imshow(V_test[:,train_idx], cmap='coolwarm')\n",
    "\n",
    "plt.figure(3, figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "plt.title('Ground Truth for Value Function on Test Set')\n",
    "plt.imshow(V[:,test_idx], cmap='coolwarm')\n",
    "plt.subplot(122)\n",
    "plt.title('Reconstruction of Value Function on Test Set')\n",
    "plt.imshow(V_test[:,test_idx], cmap='coolwarm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the models is able to correctly reconstruct the value function on the training set. However on the test set we can see that the model doesn't generalize well. I think using SVD for low rank approximation is too simple when data is missing. It would be interesting to use more advanced methods for matrix completion like [this paper](https://arxiv.org/abs/0901.3150)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe1105812d0>]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "x = [1,2,3]\n",
    "y = [4,5,6]\n",
    "plt.plot(x,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
